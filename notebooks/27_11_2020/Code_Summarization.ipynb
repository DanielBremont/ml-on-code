{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bO4r-iI8fgm"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In recent time, large, pre-trained, language models had shown high potential is several tasks. Such as Question Answering, Sentiment Analysis, Abstractive Summary etc. Some of the important models include [ELMo (Peters\n",
        "et al., 2018)](https://arxiv.org/abs/1802.05365), [GPT (Radford et al., 2018)](https://arxiv.org/abs/2005.14165), [BERT (Devlin et al., 2018)](https://arxiv.org/pdf/1810.04805.pdf), [XLNet (Yang et al., 2019)](https://arxiv.org/pdf/1906.08237.pdf), and [RoBERTa (Liu et al., 2019)](https://arxiv.org/pdf/1907.11692.pdf).\n",
        "\n",
        "They all follow the base architecture proposed by Vaswani et. al. in their Seminal Paper: [Attention is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "Following the The naturalness hypothesis of source code, proposed by Allamanis et. al. it is thus preferable to try to treat large code corpora in the similar fashion and exploit their Statistical Properties. \n",
        "\n",
        "In this colab, we will see an end to end pipeline, using [Hugging Face Transformers Library](https://huggingface.co/transformers/), [Microsoft's Open Source Large Scale code model CodeBERT](https://huggingface.co/microsoft/codebert-base), and [Codist tree-hugger](https://github.com/autosoft-dev/tree-hugger) how to use similar technology to a very challenging problem called Code Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot7DoEZMTdNs"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Microsoft Research Asia working together with Developer Division and Bing introduce [CodeXGLUE](https://github.com/microsoft/CodeXGLUE), a **benchmark dataset and open challenge for code intelligence**.\n",
        "\n",
        "It includes 14 datasets ([CodeSearchNet](https://github.com/github/CodeSearchNet), [Py150](https://eth-sri.github.io/py150)...) for 10 diversified code intelligence tasks. Those datasets are all created from Open Source repos. CodeXGLUE also includes baseline model implementations.\n",
        "\n",
        "CodeXGLUE is for code what ImageNet is for Computer Vision or GLUE for NLP. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**ü§î BUT**\n",
        "\n",
        "What if you want to **add your own dataset to these pre-built ones** or **test the baseline models on your code**?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjmjNVmcWso3"
      },
      "source": [
        "## tree-hugger: code pre-processing library\n",
        "\n",
        "At Codist, we recently open sourced our **code processing library** [tree-hugger](https://github.com/autosoft-dev/tree-hugger). In this tutorial we will show you how to :\n",
        "* install and set tree-hugger code processing library\n",
        "* create your own dataset similar to the Open Source dataset supplied by CodeXGLUE\n",
        "\n",
        "\n",
        "\n",
        "üèÜ You can then **test the baseline model** on your own data and see how it performs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wQw_15yY1Vf"
      },
      "source": [
        "### Let's install tree-hugger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCSFPT6xYtkA"
      },
      "source": [
        "!pip install -U tree-hugger PyYAML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7I6O9LHZBtt"
      },
      "source": [
        "### And use this command to build the necessary processing libary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bgf9OYFTBgZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "49c98dc0-7cf4-4214-ab65-699727c21d4d"
      },
      "source": [
        "!create_libs -c python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-10 10:53:52,071 INFO:Cloneing python repo from tree-sitter collections\n",
            "2020-10-10 10:54:02,856 INFO:Creating the library my-languages.so at /content\n",
            "2020-10-10 10:54:03,668 INFO:Finished creating library!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbRAiCdjdkqE"
      },
      "source": [
        "Now that we have all the necessary set-up done, let's download some files. For the ease of the tutorial we have created a small github example repo with a collection of files. Some of it is coming from Open Source repos and some we created as example files. \n",
        "\n",
        "Let's clone that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sVdIc1idhcj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "f07c8388-3ea3-48b2-f47c-650ac142c46b"
      },
      "source": [
        "!git clone https://github.com/autosoft-dev/example-files.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'example-files'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 16 (delta 2), reused 11 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHM0NAnCeLaf"
      },
      "source": [
        "We are going to declare a small function that will help us go over each files in a nested directory tree (like the one above we cloned) and get each file at a time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cWsYhNyeIb5"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def check_out_path(target_path: Path):\n",
        "    \"\"\"\"\n",
        "    This function recursively yields all contents of a pathlib.Path object\n",
        "    \"\"\"\n",
        "    yield target_path\n",
        "    for file in target_path.iterdir():\n",
        "        if file.is_dir():\n",
        "            yield from check_out_path(file)\n",
        "        else:\n",
        "            yield file.absolute()\n",
        "\n",
        "\n",
        "def is_python_file(file_path: Path):\n",
        "  \"\"\"\n",
        "  This little function will help us to filter the result and keep only the python files\n",
        "  \"\"\"\n",
        "  return file_path.is_file() and file_path.suffix == \".py\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB6OniXlenyC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "40e65d9a-e5e7-442a-fa69-bc159d513255"
      },
      "source": [
        "for file_path in check_out_path(Path(\"example-files\")):\n",
        "  if is_python_file(file_path):\n",
        "    print(file_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/example-files/simple_funcs/simple_funcs.py\n",
            "/content/example-files/flask_files/cli.py\n",
            "/content/example-files/api.py\n",
            "/content/example-files/inner_dir/_internal_utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU92tn86f93x"
      },
      "source": [
        "And now, we will define another small function, which, given a string which represents Python code will tokeize that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud727MLOfyUY"
      },
      "source": [
        "from tokenize import tokenize\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "def tokenize_code_string(text):\n",
        "    code_tokens = []\n",
        "    for tok in tokenize(BytesIO(text.encode('utf-8')).readline):\n",
        "        if tok.string.strip() != \"\" and tok.string.strip() != \"utf-8\":\n",
        "            code_tokens.append(tok.string.strip().lower())\n",
        "    return code_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxp1hwkWgKOQ"
      },
      "source": [
        "That is all for the pre-processing. Let's use tree-hugger's powerful API in conjunction with those functions to define a dataset from those custom files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTQTIPvFexFo"
      },
      "source": [
        "# We first create our PythonParser object\n",
        "from tree_hugger.core import PythonParser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrlIUSCBgecv"
      },
      "source": [
        "pp = PythonParser(library_loc=\"/content/my-languages.so\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8Eyii31gjk1"
      },
      "source": [
        "# We will now define a dict and populate it with the necessary data in a for loop\n",
        "\n",
        "final_out_put_data = {}\n",
        "\n",
        "for file_path in check_out_path(Path(\"example-files\")):\n",
        "  if is_python_file(file_path):\n",
        "    final_out_put_data[file_path.stem] = None\n",
        "    # we use one line, super convinient tree-hugger API call to get the needed data\n",
        "    if pp.parse_file(str(file_path)):\n",
        "      temp_cache = []\n",
        "      # The following call returns a dict where each key is a name of a function\n",
        "      # And each value is a tuple, (function_body, function_docstring)\n",
        "      func_and_docstr = pp.get_all_function_bodies(strip_docstr=True)\n",
        "      for func_name, (body, docstr) in func_and_docstr.items():\n",
        "        code_tokens = tokenize_code_string(body)\n",
        "        # Let's strip out all the internal comments\n",
        "        final_code_tokens = [t for t in code_tokens if not t.startswith(\"#\")]\n",
        "        # Split the first line of docstring and remove all the tripple quotes and strip white spaces and make it lower\n",
        "        docstr_tokens = docstr.split(\"\\n\")[0].strip().replace('\"\"\"', '').replace(\"'''\", \"\").lower().split()\n",
        "        temp_cache.append({\"code\": final_code_tokens, \"docstr\": docstr_tokens})\n",
        "      # Let's add the result to the final output\n",
        "      final_out_put_data[file_path.stem] = temp_cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i9vdl2nlLQO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "outputId": "f96ecf71-f205-4de2-d573-8356f9a0249f"
      },
      "source": [
        "# And we are DONE!\n",
        "\n",
        "final_out_put_data[\"api\"][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'code': ['def',\n",
              "  'request',\n",
              "  '(',\n",
              "  'method',\n",
              "  ',',\n",
              "  'url',\n",
              "  ')',\n",
              "  ':',\n",
              "  'with',\n",
              "  'sessions',\n",
              "  '.',\n",
              "  'session',\n",
              "  '(',\n",
              "  ')',\n",
              "  'as',\n",
              "  'session',\n",
              "  ':',\n",
              "  'return',\n",
              "  'session',\n",
              "  '.',\n",
              "  'request',\n",
              "  '(',\n",
              "  'method',\n",
              "  '=',\n",
              "  'method',\n",
              "  ',',\n",
              "  'url',\n",
              "  '=',\n",
              "  'url',\n",
              "  ',',\n",
              "  '**',\n",
              "  'kwargs',\n",
              "  ')'],\n",
              " 'docstr': ['constructs',\n",
              "  'and',\n",
              "  'sends',\n",
              "  'a',\n",
              "  ':class:`request',\n",
              "  '<request>`.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM38KwJ5mzH0"
      },
      "source": [
        "With this code, you can very easily create a dataset out of your own code files and then test the baseline models against it. \n",
        "\n",
        "\n",
        "That was easy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYpSBwH11Shw"
      },
      "source": [
        " (We are about to release `docly` a small command line tool which helps you to write function documentation for your Python code and we use the same parsing technique there as well üòÄ )"
      ]
    }
  ]
}