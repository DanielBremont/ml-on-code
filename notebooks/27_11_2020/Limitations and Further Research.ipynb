{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Set of Limitations, in the chosen modelling technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In their recent paper [\"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference\"](https://www.aclweb.org/anthology/P19-1334/) McCoy et. al. has proposed a new Benchmarking Data Set HANS(Heuristic Analysis for NLI Systems) and showed how SOTA language models, such as BERT, performs very poorly on it. Arguing that, BERT (and similar other ones) largely depends on three types of surface level heuristic, e.g. the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. Any medtohd (including the one we demonstrated) based on the similar ideas have a high chance of having the same issues underlying \n",
    "\n",
    "To illustrate the above point, I will show the following example.(code is simplified pseudocode for this purpose)\n",
    "\n",
    "```Python\n",
    "docstring = model(\"def add_tensors(t, t1) -> Any:\\n    return t + t1\")\n",
    "print(docstring)\n",
    "```\n",
    "\n",
    "Ouputs \n",
    "`['Add two tensors .']`\n",
    "\n",
    "And when changed to the following\n",
    "\n",
    "```Python\n",
    "docstring = model(\"def add_tensors(t, t1) -> Any:\\n    return t * t1\")\n",
    "print(docstring)\n",
    "```\n",
    "\n",
    "It **ALSO** outputs the following\n",
    "`['Add two tensors .']`\n",
    "\n",
    "* In a recent [paper](https://arxiv.org/pdf/2010.01410.pdf) (one of the author of this paper is also a co-author of the paper which originally proposed Naturalness Hypothesis) \"Code to Comment “Translation”:Data, Metrics, Baselining & Evaluation\" Gros et. al. showed that in many standard data-sets code to comment translation can achieve a near SOTA performance using just naive Information retrieval approach. They also show that the characteristics of CCT (code to comment) datasets are not really similar natural translation datasets (WMT was used), which creats a large room of innovation on top of the commonly used large language pre-training models.\n",
    "\n",
    "* Gary Marcus and many others has had many a times pointed out where DL based techniues gone horribly wrong on very simple tasks, they argued the otherwise \"brittle\" nature of DL, problems with generalization, data hungry-ness, lack of common sense, and last but not the least lack of explainability. Since we are modelling code using purely DL based approaches we are also not free from any of them.\n",
    "\n",
    "* Open Vocabulary issue. Source code Vocabulary is open, in the sense that it can contain many many words that are rare or completely made up by the developer. Whereas in a NL setting we can do with more or less fixing a large Vocabulary, the same approach should fail often in case of code. In their [recent paper](https://research.google/pubs/pub48933/) \"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code\" Karampatsis et. al. treated this particular subject in details and showed some ways forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code is \"Highly Structured\"\n",
    "\n",
    "This is not a new information (I hope! :D) A source code has a very well defined grammar and a restricted set of transformation rules from one expression to another"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
