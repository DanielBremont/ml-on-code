{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Set of Limitations: The chosen modelling technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In their recent paper [\"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference\"](https://www.aclweb.org/anthology/P19-1334/) McCoy et. al. has proposed a new Benchmarking Data Set HANS(Heuristic Analysis for NLI Systems) and showed how SOTA language models, such as BERT, performs very poorly on it. Arguing that, BERT (and similar other ones) largely depends on three types of surface level heuristic, e.g. the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. Any medtohd (including the one we demonstrated) based on the similar ideas have a high chance of having the same issues underlying \n",
    "\n",
    "To illustrate the above point, I will show the following example.(code is simplified pseudocode for this purpose)\n",
    "\n",
    "```Python\n",
    "docstring = model(\"def add_tensors(t, t1) -> Any:\\n    return t + t1\")\n",
    "print(docstring)\n",
    "```\n",
    "\n",
    "Ouputs \n",
    "`['Add two tensors .']`\n",
    "\n",
    "And when changed to the following\n",
    "\n",
    "```Python\n",
    "docstring = model(\"def add_tensors(t, t1) -> Any:\\n    return t * t1\")\n",
    "print(docstring)\n",
    "```\n",
    "\n",
    "It **ALSO** outputs the following\n",
    "`['Add two tensors .']`\n",
    "\n",
    "* In their recent [paper](https://arxiv.org/pdf/2010.01410.pdf) (one of the author of this paper is also a co-author of the paper which originally proposed Naturalness Hypothesis) \"Code to Comment “Translation”:Data, Metrics, Baselining & Evaluation\" Gros et. al. showed that in many standard data-sets code to comment translation can achieve a near SOTA performance using just naive Information retrieval approach. They also show that the characteristics of CCT (code to comment) datasets are not really similar natural translation datasets (WMT was used), which creats a large room of innovation on top of the commonly used large language pre-training models.\n",
    "\n",
    "* Gary Marcus and many others has had many a times pointed out where DL based techniues gone horribly wrong on very simple tasks. They argued the otherwise \"brittle\" nature of DL, problems with generalization, data hungry-ness, lack of common sense, and last but not the least lack of explainability. Since we are modelling code using purely DL based approaches we are also not free from any of them.\n",
    "\n",
    "* Open Vocabulary issue. Source code Vocabulary is open, in the sense that it can contain many many words that are rare or completely made up by the developer. Whereas in a NL setting we can do with more or less fixing a large Vocabulary, the same approach should fail often in case of code. In their [recent paper](https://research.google/pubs/pub48933/) \"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code\" Karampatsis et. al. treated this particular subject in details and showed some ways forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code is \"Highly Structured\"\n",
    "\n",
    "This is not a new information (I hope! :D) A source code has a very well defined grammar and a restricted set of transformation rules from one expression to another. This underlying structure of code can be easily expressed using AST, DFG, CFG, or some other novel graph representation. This new represnetaition goes way beyond simple token based representations and can provide significant eaxtra power to the models. [Some](https://arxiv.org/abs/2002.09440) [recent](https://arxiv.org/abs/2009.08366) and [not so recent](https://arxiv.org/abs/1903.03804) works has been done on it. And it is a highly active field of research. \n",
    "\n",
    "We hope that mix modal modelling (graph + token based) augmented with knowledge graphs can yield a new class of results and better generalization in this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A case for Fusion\n",
    "\n",
    "Fusion energy, if made practically viable, can unleash the next generation of clean energy. Funny enough that the same can be said about AI and ML. Before the advent of Deep Learning the main research of AI was based on Symbolic AI (GOFAI as some of you may know it). It looks like that there is a new found interest where people are keen in marrying these two approaches. In his last paper [\"The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence\"](https://arxiv.org/ftp/arxiv/papers/2002/2002.06177.pdf) Gary Marcus has made a strong case for it. Also [\"Graph Neural Networks Meet Neural-Symbolic Computing:A Survey and Perspective\"](https://www.ijcai.org/Proceedings/2020/0679.pdf) by Lamb et.al. proposes solid avenues to explore in the same direction. \n",
    "\n",
    "The main argument here is the fact that the power of of symbol manipulation will give DL its missing power of \"Reasoning\". A very good work in this direction is [DeepProbLog](https://arxiv.org/abs/1907.08194) \n",
    "\n",
    "Of course not everyone is agreeing with this, Geoff Hinton [recently claimed](https://www.technologyreview.com/2020/11/03/1011616/ai-godfather-geoffrey-hinton-deep-learning-will-do-everything/) \"Deep learning is going to be able to do everything\". Whether that is the right notion or whether we need the fusion technology, only future can tell. \n",
    "\n",
    "\n",
    "At Codist we are experimenting with these frontier technological aspects. We often work in areas where very little amount of research can be found (How to help SAT solvers using DL? Neuro Symbolic Computing etc.) and bringing the results in form of products that we as developers love to use, and we hope that many others share the same love. We have released `docly` [recently](https://codist-ai.com/). This is just the begining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
